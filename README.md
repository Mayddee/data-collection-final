
üåü Project Overview

This project demonstrates a **complete streaming + batch data pipeline** using **Python, Apache Airflow, Kafka, and SQLite**.  
It covers the **full lifecycle of real-time data**: ingestion, cleaning, storage, and analytical aggregation.

 Airflow DAGs

1. Continuous ingestion: WazirX API ‚Üí Kafka  
2. Hourly batch processing: Kafka ‚Üí SQLite (events table)  
3. Daily analytics: SQLite ‚Üí aggregated summary (daily_summary table)  

---

 üîó API Selection

WazirX Cryptocurrency Tickers API
Endpoint: [https://api.wazirx.com/api/v2/tickers](https://api.wazirx.com/api/v2/tickers)

Why WazirX?

- Frequently updated (prices change multiple times per hour)  
- Stable, documented, and widely used  
- Structured JSON output  
- Returns real trading data: prices, volumes, timestamps  

---

 üèóÔ∏è System Architecture

Lambda-style pipeline:

```

WazirX API ‚Üí Airflow DAG 1 ‚Üí Kafka (raw_events) ‚Üí Airflow DAG 2 ‚Üí SQLite (events) ‚Üí Airflow DAG 3 ‚Üí SQLite (daily_summary)



## ‚ö° How to Run the Project

### 1Ô∏è‚É£ Create & Activate Virtual Environment
```bash
python3 -m venv .venv
source .venv/bin/activate
````

### 2Ô∏è‚É£ Install Dependencies

```bash
pip install -r requirements.txt
```

### 3Ô∏è‚É£ Start Kafka & Zookeeper

```bash
docker-compose up -d
```

* Bootstrap server: `localhost:9092`
* Kafka topic: `raw_events`

### 4Ô∏è‚É£ Run Scripts (Optional for Testing)

```bash
python src/job1_producer.py
python src/job2_cleaner.py
python src/job3_analytics.py
```

### 5Ô∏è‚É£ Run Airflow

#### Standalone Mode

```bash
export AIRFLOW_HOME=$(pwd)/airflow
airflow standalone
```

Access Web UI: [http://localhost:8080](http://localhost:8080)

#### Separate Components

* **Scheduler:**

```bash
export AIRFLOW_HOME=$(pwd)/airflow
airflow scheduler
```

* **Webserver:**

```bash
export AIRFLOW_HOME=$(pwd)/airflow
airflow webserver -p 8080
```

---

## üìä SQLite Schema

**events Table**

| Column       | Type      | Description         |
| ------------ | --------- | ------------------- |
| id           | INTEGER   | Primary key         |
| symbol       | TEXT      | Trading pair symbol |
| last_price   | REAL      | Last traded price   |
| open_price   | REAL      | Opening price       |
| high_price   | REAL      | Highest price       |
| low_price    | REAL      | Lowest price        |
| volume       | REAL      | Trading volume      |
| quote_volume | REAL      | Quote asset volume  |
| event_time   | TIMESTAMP | Event timestamp     |

**daily_summary Table**

| Column       | Type    | Description         |
| ------------ | ------- | ------------------- |
| summary_date | DATE    | Aggregation date    |
| symbol       | TEXT    | Trading pair symbol |
| avg_price    | REAL    | Average daily price |
| min_price    | REAL    | Minimum daily price |
| max_price    | REAL    | Maximum daily price |
| total_volume | REAL    | Total daily volume  |
| record_count | INTEGER | Number of records   |



## üõ†Ô∏è Technologies Used

* **Python**
* **Apache Kafka**
* **Apache Airflow**
* **SQLite**
* **Pandas**
* **WazirX Cryptocurrency Tickers API** ‚Äì [https://api.wazirx.com/api/v2/tickers](https://api.wazirx.com/api/v2/tickers)




## Notes

* Ensure Kafka is running before starting Airflow DAGs.
* Optional: test scripts individually before running scheduled DAGs.
* All times are in UTC by default.


